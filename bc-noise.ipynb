{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Note: this notebook is taken from the tutorial provided on the SB3 website. Go there for more information and documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyyN-2qyK_T2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stable Baselines3 Tutorial - Getting Started\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
    "\n",
    "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
    "\n",
    "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "\n",
    "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
    "\n",
    "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip\n",
    "\n",
    "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
    "\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Feel free to disable GPU if you don't have one\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U29X1-B-AIKE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.1a0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "stable_baselines3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtY8FhliLsGm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcX8hEcaUpR0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Stable-Baselines works on environments that follow the [gym interface](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html).\n",
    "You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n",
    "\n",
    "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
    "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines.readthedocs.io/en/master/guide/algos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BIedd7Pz9sOs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae32CtgzTG3R",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R7tKaBFrTR0a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0_8OQbOTTNT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
    "This step is optional as you can directly use strings in the constructor: \n",
    "\n",
    "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
    "\n",
    "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ROUJr675TT01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RapkYvTXL7Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the Gym env and instantiate the agent\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
    "\n",
    "Note: vectorized environments allow to easily multiprocess training. In this example, we are using only one process, hence the DummyVecEnv.\n",
    "\n",
    "We chose the MlpPolicy because input of CartPole is a feature vector, not images.\n",
    "\n",
    "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
    "\n",
    "\n",
    "Here we are using the [Proximal Policy Optimization](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
    "\n",
    "It combines ideas from [A2C](https://stable-baselines.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
    "\n",
    "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
    "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pUWGZp3i9wyf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4efFdrQ7MBvl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We create a helper function to evaluate the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "63M8mSKR-6Zt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=100, deterministic=True):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs, deterministic=deterministic)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hkyafs--gJz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In fact, Stable-Baselines3 already provides you with that helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s6ZNldIR-fce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjEVOIY8NVeK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's evaluate the un-trained agent, this should be a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xDHLMA6NFk95",
    "outputId": "a7115c9f-1076-48a2-a78b-61d387609c5f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/.local/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:9.12 +/- 0.70\n"
     ]
    }
   ],
   "source": [
    "# Use a separate environement for evaluation\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5UoXTZPNdFE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the agent and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e4cfSXIB-pTF",
    "outputId": "8f6fe592-df0a-4731-f447-b7c059b4f500",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7feb082ee890>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ygl_gVmV_QP7",
    "outputId": "e2e2a301-bfea-48a4-c046-2912d9a90124",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:314.83 +/- 128.37\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A00W6yY3NkHG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Apparently the training went well, the mean reward increased a lot ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVm9QPNVwKXN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MPyfQxD5z26J",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SLzXxO8VMD6N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    "  \"\"\"\n",
    "  Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "  :param video_path: (str) Path to the folder containing videos\n",
    "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "  \"\"\"\n",
    "  html = []\n",
    "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "      html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTRNUfulOGaF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Trag9dQpOIhx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "  \"\"\"\n",
    "  :param env_id: (str)\n",
    "  :param model: (RL model)\n",
    "  :param video_length: (int)\n",
    "  :param prefix: (str)\n",
    "  :param video_folder: (str)\n",
    "  \"\"\"\n",
    "  eval_env = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n",
    "  # Start the video at step=0 and record 500 steps\n",
    "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "  obs = eval_env.reset()\n",
    "  for _ in range(video_length):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "  # Close the video recorder\n",
    "  eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOObbeu5MMlR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize trained agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iATu7AiyMQW2",
    "outputId": "7a1ffe06-b8c5-42e5-b075-1b04b98c017e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyopengl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyopengl\u001b[39;00m\n\u001b[1;32m      2\u001b[0m record_video(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, model, video_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo-cartpole\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyopengl'"
     ]
    }
   ],
   "source": [
    "import pyopengl\n",
    "record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-n4i-fW3NojZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"videos/ppo-cartpole-step-0-to-step-500.mp4\" autoplay \n",
       "                    loop controls style=\"height: 400px;\">\n",
       "                    <source src=\"data:video/mp4;base64,\" type=\"video/mp4\" />\n",
       "                </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_videos('videos', prefix='ppo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y8zg4V566qD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bonus: Train a RL Model in One Line\n",
    "\n",
    "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iaOPfOrwWEP4",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.3     |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 1757     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.5        |\n",
      "|    ep_rew_mean          | 25.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1200        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006990035 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.00254     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.98        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 58.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.6        |\n",
      "|    ep_rew_mean          | 31.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1086        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007937998 |\n",
      "|    clip_fraction        | 0.0701      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0984      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 36.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 46.4        |\n",
      "|    ep_rew_mean          | 46.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1036        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008865889 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 50.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60.9        |\n",
      "|    ep_rew_mean          | 60.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1009        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009042452 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 63.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 77.1        |\n",
      "|    ep_rew_mean          | 77.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 991         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005300204 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00971    |\n",
      "|    value_loss           | 76.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94.4        |\n",
      "|    ep_rew_mean          | 94.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 979         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005531225 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.48        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00912    |\n",
      "|    value_loss           | 44.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 113          |\n",
      "|    ep_rew_mean          | 113          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 970          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039496776 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | 0.639        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.6         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 46.3         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 134        |\n",
      "|    ep_rew_mean          | 134        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 963        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00549409 |\n",
      "|    clip_fraction        | 0.0406     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.574     |\n",
      "|    explained_variance   | 0.809      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.4       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00521   |\n",
      "|    value_loss           | 33.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 151         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 957         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009992357 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.444       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 170          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 953          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019962036 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.0341       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16           |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000143    |\n",
      "|    value_loss           | 68.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 188         |\n",
      "|    ep_rew_mean          | 188         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 949         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009094384 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.0808      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.211       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 6.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 207         |\n",
      "|    ep_rew_mean          | 207         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 946         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009414369 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    value_loss           | 4.1         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 224          |\n",
      "|    ep_rew_mean          | 224          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 943          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056674452 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.513       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.122        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 2.36         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 243         |\n",
      "|    ep_rew_mean          | 243         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 941         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006061474 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0217      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 262          |\n",
      "|    ep_rew_mean          | 262          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 939          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052424334 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.506       |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.126        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00639     |\n",
      "|    value_loss           | 1.01         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 283         |\n",
      "|    ep_rew_mean          | 283         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 937         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006614673 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    value_loss           | 0.578       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 300         |\n",
      "|    ep_rew_mean          | 300         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 936         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003084714 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.0356      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 0.363       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 316          |\n",
      "|    ep_rew_mean          | 316          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 934          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025169817 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.509       |\n",
      "|    explained_variance   | 0.172        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0285       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    value_loss           | 0.217        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 333         |\n",
      "|    ep_rew_mean          | 333         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 933         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001592543 |\n",
      "|    clip_fraction        | 0.0129      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.0193      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00304     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.000122    |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 351          |\n",
      "|    ep_rew_mean          | 351          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 932          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022083498 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.0542       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0147       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 0.0865       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 367         |\n",
      "|    ep_rew_mean          | 367         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 931         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003331015 |\n",
      "|    clip_fraction        | 0.0299      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0015      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    value_loss           | 0.0545      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 384         |\n",
      "|    ep_rew_mean          | 384         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 930         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001137421 |\n",
      "|    clip_fraction        | 0.00596     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | -0.0174     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00277     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 6.01e-05    |\n",
      "|    value_loss           | 0.032       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 399         |\n",
      "|    ep_rew_mean          | 399         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 929         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006763018 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | -0.0434     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00204    |\n",
      "|    value_loss           | 0.0215      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 414         |\n",
      "|    ep_rew_mean          | 414         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 929         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004670736 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.0921      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00927    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    value_loss           | 0.0142      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 429          |\n",
      "|    ep_rew_mean          | 429          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 928          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026390732 |\n",
      "|    clip_fraction        | 0.0114       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.502       |\n",
      "|    explained_variance   | -0.0405      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0138       |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000471    |\n",
      "|    value_loss           | 0.0093       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 440         |\n",
      "|    ep_rew_mean          | 440         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 927         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005488293 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.0378      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00201     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00145    |\n",
      "|    value_loss           | 0.00582     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 455          |\n",
      "|    ep_rew_mean          | 455          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 927          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013299331 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.51        |\n",
      "|    explained_variance   | 0.0116       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000917     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 0.00397      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 467          |\n",
      "|    ep_rew_mean          | 467          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 926          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029116173 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | -0.0174      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00995     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 476          |\n",
      "|    ep_rew_mean          | 476          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 925          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069559584 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | -0.032       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.015        |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 0.00177      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 485         |\n",
      "|    ep_rew_mean          | 485         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 925         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003914877 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.047       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00233     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 492          |\n",
      "|    ep_rew_mean          | 492          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015656172 |\n",
      "|    clip_fraction        | 0.00313      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.504       |\n",
      "|    explained_variance   | 0.0152       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0137      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000171    |\n",
      "|    value_loss           | 0.000874     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 494          |\n",
      "|    ep_rew_mean          | 494          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012883171 |\n",
      "|    clip_fraction        | 0.00913      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.509       |\n",
      "|    explained_variance   | 0.0159       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00877      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -4.8e-05     |\n",
      "|    value_loss           | 0.000565     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 498          |\n",
      "|    ep_rew_mean          | 498          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014822148 |\n",
      "|    clip_fraction        | 0.0137       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.508       |\n",
      "|    explained_variance   | 0.0126       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00166     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | 0.000119     |\n",
      "|    value_loss           | 0.000428     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038854838 |\n",
      "|    clip_fraction        | 0.0667       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.489       |\n",
      "|    explained_variance   | 0.00726      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000206     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    value_loss           | 0.000297     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022444963 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | -0.277       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00519     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000518    |\n",
      "|    value_loss           | 0.000221     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 500         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008041923 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.0111      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 0.000145    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056370925 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | -0.155       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00369      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 0.000125     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 500         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003552769 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | -0.217      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00255    |\n",
      "|    value_loss           | 8.91e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 921          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038437175 |\n",
      "|    clip_fraction        | 0.0558       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | -0.209       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00922     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00286     |\n",
      "|    value_loss           | 7.31e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 500         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 921         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004712672 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | -0.0657     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0242      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 3.92e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 921          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048274435 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | -0.266       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00998     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 2.58e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 920          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047684824 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.437       |\n",
      "|    explained_variance   | 0.0717       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00745      |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    value_loss           | 2.31e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 920          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037292938 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.432       |\n",
      "|    explained_variance   | -0.154       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00712     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    value_loss           | 1.35e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 920          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049935738 |\n",
      "|    clip_fraction        | 0.0798       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | -0.126       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00806     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    value_loss           | 1.26e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 919          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023376485 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.423       |\n",
      "|    explained_variance   | 0.0165       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00971     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 9.29e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 919          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048732758 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | -0.301       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0116       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000886    |\n",
      "|    value_loss           | 1e-05        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 919          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 106          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028415113 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.0433       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00696     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000704    |\n",
      "|    value_loss           | 5.61e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 919          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028990014 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | -0.384       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00202      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000883    |\n",
      "|    value_loss           | 5.21e-06     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Additional tasks:\n",
    "\n",
    "1. Change the environment\n",
    "2. Benchmark two separate algorithms in this new environment.\n",
    "3. (Optional) Play with hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2: Behavioral Cloning\n",
    "\n",
    "Here, you'll implement a deep learning algorithm that can store multiple episodes. We'll do it from scratch so that it'll be easier to perturb the individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Step 1: Make a data structure to store transitions.\n",
    "### At the very least, you'll need observations and actions.\n",
    "\n",
    "# example implementation\n",
    "class TransitionStorage:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.action = []\n",
    "\n",
    "    def store_transition(self, obs, action):\n",
    "        \"\"\"\n",
    "        update self.obs and self.action based on obs and action\n",
    "        \"\"\"\n",
    "        # added 50% of noise with random action\n",
    "        if np.random.choice([0, 1]) == 0:\n",
    "            if action == 0:\n",
    "                self.action.append([1, 0])\n",
    "            else:\n",
    "                self.action.append([0, 1])\n",
    "        else:\n",
    "            rand_action = np.random.choice([0, 1])\n",
    "            if rand_action == 0:\n",
    "                self.action.append([1, 0])\n",
    "            else:\n",
    "                self.action.append([0, 1])\n",
    "        self.obs.append(obs)\n",
    "    def get_batch(self, batch_size):\n",
    "        curr = 0\n",
    "        while len(self.obs) >= curr:\n",
    "            yield self.obs[curr:curr+batch_size], self.action[curr:curr+batch_size]\n",
    "            curr += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Step 2: Reuse the evaluate function, but now use it to collect data\n",
    "\n",
    "def evaluate_and_collect(model, storage, num_episodes=100, deterministic=True):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs, deterministic=deterministic)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # store the transition\n",
    "            storage.store_transition(obs, action)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7feb0814be80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 500.0 Num episodes: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step 3: Collect some data on an expert.\n",
    "### You can use the evaluate_and_collect function you wrote above.\n",
    "\n",
    "storage = TransitionStorage()\n",
    "evaluate_and_collect(model, storage, num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Step 4: Define a network that you'll train through behavioral cloning (supervised learning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# BCNetwork but with a discrete action space\n",
    "class BCNetworkDiscrete(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        # assumes that observation and action are one-dimensional\n",
    "        super(BCNetworkDiscrete, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.obs_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, self.action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=2) # what should you do to x to make it a probability distribution?\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5873040199813843\n",
      "Epoch: 1, Loss: 0.5731050832233429\n",
      "Epoch: 2, Loss: 0.5700872869758606\n",
      "Epoch: 3, Loss: 0.5692487154121398\n",
      "Epoch: 4, Loss: 0.5686751751441955\n",
      "Epoch: 5, Loss: 0.5681885942802429\n",
      "Epoch: 6, Loss: 0.5681287897510529\n",
      "Epoch: 7, Loss: 0.5678863709144593\n",
      "Epoch: 8, Loss: 0.5676791990032196\n",
      "Epoch: 9, Loss: 0.5675071712188721\n"
     ]
    }
   ],
   "source": [
    "### Step 5: Train the network\n",
    "\n",
    "# initialize the network\n",
    "network = BCNetworkDiscrete(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# define the number of batches\n",
    "num_batches = len(storage.obs) // batch_size\n",
    "\n",
    "# note: you can keep the obs and action completely in memory \n",
    "# (the training loop is set up for that). make sure that:\n",
    "# 1. the types are correct\n",
    "# 2. the shapes match\n",
    "\n",
    "# train the network\n",
    "for epoch in range(num_epochs):\n",
    "    gen = storage.get_batch(batch_size)\n",
    "    # accumulate loss\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # get the batch somehow. you can either write a method \n",
    "        # into the storage class or just directly access the \n",
    "        # values in it\n",
    "        \n",
    "        batch_obs, batch_action = next(gen)\n",
    "        batch_obs, batch_action = torch.FloatTensor(batch_obs).to(device), torch.FloatTensor(batch_action).to(device)\n",
    "        # forward pass\n",
    "        logits = network(batch_obs)\n",
    "        # need to squeeze out the extra dimension\n",
    "        logits = torch.squeeze(logits)\n",
    "    \n",
    "        # compute the loss\n",
    "        loss = loss_fn(logits, batch_action)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch, epoch_loss / num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train the network\n",
    "for epoch in range(num_epochs):\n",
    "    gen = storage.get_batch(batch_size)\n",
    "    # accumulate loss\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # get the batch somehow. you can either write a method \n",
    "        # into the storage class or just directly access the \n",
    "        # values in it\n",
    "        \n",
    "        batch_obs, batch_action = next(gen)\n",
    "        batch_obs, batch_action = torch.FloatTensor(batch_obs), torch.FloatTensor(batch_action)\n",
    "        # forward pass\n",
    "        logits = network(batch_obs)\n",
    "        # need to squeeze out the extra dimension\n",
    "        logits = torch.squeeze(logits)\n",
    "    \n",
    "        # compute the loss\n",
    "        loss = loss_fn(logits, batch_action)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    # print the loss\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch, epoch_loss / num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 500.0 Num episodes: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step 6: run the trained network on the environment, based on the evaluate function but using network instead of model\n",
    "def evaluate_network(network, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            \n",
    "            # need to add the additional dimenstion becuase of the \n",
    "            # single batch training\n",
    "            action = network(torch.tensor([obs], dtype=torch.float32).to(device)).to(device).argmax().item()\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step([action])\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward\n",
    "\n",
    "evaluate_network(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     action \u001b[38;5;241m=\u001b[39m network(torch\u001b[38;5;241m.\u001b[39mtensor([obs], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      9\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep([action])\n\u001b[0;32m---> 10\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# save the video\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:87\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mGym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m:param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:179\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m cartheight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30.0\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rendering\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m rendering\u001b[38;5;241m.\u001b[39mViewer(screen_width, screen_height)\n\u001b[1;32m    182\u001b[0m     l, r, t, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcartwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cartwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cartheight \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39mcartheight \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:27\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Cannot import pyglet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Error occurred while running `from pyglet.gl import *`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/gl/__init__.py:226\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pyglet_doc_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyglet.window\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;129;01mand\u001b[39;00m _pyglet\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshadow_window\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# trickery is for circular import\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     _pyglet\u001b[38;5;241m.\u001b[39mgl \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/window/__init__.py:1915\u001b[0m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pyglet_doc_run:\n\u001b[1;32m   1914\u001b[0m     pyglet\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m-> 1915\u001b[0m     \u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_shadow_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/gl/__init__.py:200\u001b[0m, in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124;03m\"\"\"Shadow window does not need a projection.\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m _shadow_window \u001b[38;5;241m=\u001b[39m \u001b[43mShadowWindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m _shadow_window\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m app\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/gl/__init__.py:194\u001b[0m, in \u001b[0;36m_create_shadow_window.<locals>.ShadowWindow.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/window/xlib/__init__.py:168\u001b[0m, in \u001b[0;36mXlibWindow.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_handlers[message] \u001b[38;5;241m=\u001b[39m func\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mXlibWindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _can_detect_autorepeat\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_detect_autorepeat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/window/__init__.py:548\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_queue \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m display:\n\u001b[0;32m--> 548\u001b[0m     display \u001b[38;5;241m=\u001b[39m \u001b[43mpyglet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m screen:\n\u001b[1;32m    551\u001b[0m     screen \u001b[38;5;241m=\u001b[39m display\u001b[38;5;241m.\u001b[39mget_default_screen()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/canvas/__init__.py:94\u001b[0m, in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m display\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Otherwise, create a new display and return it.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/canvas/xlib.py:123\u001b[0m, in \u001b[0;36mXlibDisplay.__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display \u001b[38;5;241m=\u001b[39m xlib\u001b[38;5;241m.\u001b[39mXOpenDisplay(name)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDisplayException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot connect to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m    125\u001b[0m screen_count \u001b[38;5;241m=\u001b[39m xlib\u001b[38;5;241m.\u001b[39mXScreenCount(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_screen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m screen_count:\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "### Step 7: visualize the new policy, and save to an mp4 file\n",
    "\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "frames = []\n",
    "while not done:\n",
    "    action = network(torch.tensor([obs], dtype=torch.float32).to(device)).argmax().item()\n",
    "    obs, reward, done, info = env.step([action])\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "env.close()\n",
    "\n",
    "# save the video\n",
    "import imageio  # note: pip install imageio[ffmpeg]\n",
    "imageio.mimsave('bc.mp4', frames, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Messing around with BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write (or reuse) a loop and add noise to the policy actions. What happens?\n",
    "# less stable, but still performs pretty goood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add noise to the observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional): Try collecting data from a trained policy while applying noise, \n",
    "# then try training a new policy on that data. How does the new policy fare?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1_getting_started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
